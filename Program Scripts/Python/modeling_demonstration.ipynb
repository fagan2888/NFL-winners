{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import pandas as pds\n",
    "\n",
    "import sqlalchemy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as scipy\n",
    "\n",
    "import statistics as stats\n",
    "\n",
    "import math as math\n",
    "\n",
    "import yellowbrick as yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Database Engine and Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engine = sqlalchemy.create_engine('postgresql+psycopg2://postgres:georgetown@nflstats.cb6meldrm5db.us-east-1.rds.amazonaws.com:5432/nfl_stats', pool_recycle=3600);\n",
    "\n",
    "dbConnection = Engine.connect();\n",
    "\n",
    "df_table = pds.read_sql(\"\"\"select * from final_table_joined\"\"\", dbConnection);\n",
    "\n",
    "# Use for testing 2018 data vs prior years\n",
    "#df_table_train = pds.read_sql(\"\"\"select * from game_data_2000_to_2017\"\"\", dbConnection);\n",
    "#df_table_test = pds.read_sql(\"\"\"select * from game_data_2018\"\"\", dbConnection);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_table.describe()\n",
    "\n",
    "# Use for testing 2018 data vs prior years\n",
    "#df_table_train.describe()\n",
    "#df_table_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.head(10)\n",
    "\n",
    "# Use for testing 2018 data vs prior years\n",
    "#df_table_train.head(10)\n",
    "#df_table_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.isna().sum()\n",
    "\n",
    "# Use for testing 2018 data vs prior years\n",
    "#df_table_train.isna().sum()\n",
    "#df_table_test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['home_point_differential', 'home_passing_yards', 'home_rushing_yards', 'home_turnover_differential', 'home_passing_yards_against', 'home_rushing_yards_against', 'home_win_percentage', 'home_power_ranking','away_point_differential', 'away_passing_yards', 'away_rushing_yards', 'away_turnover_differential', 'away_passing_yards_against', 'away_rushing_yards_against', 'away_win_percentage', 'away_power_ranking']\n",
    "\n",
    "x = df_table[features].values\n",
    "y = df_table['home_outcome'].values\n",
    "\n",
    "# Use for testing 2018 data vs prior years\n",
    "#X_train = df_table_train[features].values\n",
    "#y_train = df_table_train['home_outcome'].values\n",
    "#X_test = df_table_test[features].values\n",
    "#y_test = df_table_test['home_outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "# Create train and test splits\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "# Declare model\n",
    "DTCmodel = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, min_samples_split = 0.2)\n",
    "\n",
    "# State Classes\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(DTCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(DTCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Instantiate the ROC visualizer with the classification model\n",
    "roc_visualizer = ROCAUC(DTCmodel, classes=classes)\n",
    "\n",
    "roc_visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_visualizer.show()                       # Finalize and show the figure\n",
    "\n",
    "#The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(DTCmodel, classes=classes, cmap='BuPu')\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()\n",
    "\n",
    "# Create the Precision-Recall visualizer, fit, score, and show it\n",
    "prc_viz = PrecisionRecallCurve(DTCmodel)\n",
    "prc_viz.fit(X_train, y_train)\n",
    "prc_viz.score(X_test, y_test)\n",
    "prc_viz.show()\n",
    "\n",
    "# Instantiate the classification model and Class Prediction Error visualizer\n",
    "cpe_visualizer = ClassPredictionError(\n",
    "    DTCmodel, classes=classes\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "cpe_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "cpe_visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "cpe_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Declare model\n",
    "DTCoptimal = DecisionTreeClassifier()\n",
    "\n",
    "# Specify Parameters\n",
    "min_samples_split = np.linspace(0.1,0.5,5)\n",
    "max_depth = [int(x) for x in np.linspace(1,10,1)]\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "# Define Parameter Grid\n",
    "params = dict(max_depth = max_depth,  \n",
    "             min_samples_split = min_samples_split,  \n",
    "             criterion = criterion)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "# Grid Search and Find Optimal Parameter Values\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(DTCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, DTCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "GNBmodel = GaussianNB()\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(GNBmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(GNBmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Instantiate the ROC visualizer with the classification model\n",
    "roc_visualizer = ROCAUC(GNBmodel, classes=classes)\n",
    "\n",
    "roc_visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_visualizer.show()                       # Finalize and show the figure\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(GNBmodel, classes=classes, cmap='BuPu')\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()\n",
    "\n",
    "# Create the Precision-Recall visualizer, fit, score, and show it\n",
    "prc_viz = PrecisionRecallCurve(GNBmodel)\n",
    "prc_viz.fit(X_train, y_train)\n",
    "prc_viz.score(X_test, y_test)\n",
    "prc_viz.show()\n",
    "\n",
    "# Instantiate the classification model and Class Prediction Error visualizer\n",
    "cpe_visualizer = ClassPredictionError(\n",
    "    GNBmodel, classes=classes\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "cpe_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "cpe_visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "cpe_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "RFCmodel = RandomForestClassifier()\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(RFCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(RFCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Instantiate the ROC visualizer with the classification model\n",
    "roc_visualizer = ROCAUC(RFCmodel, classes=classes)\n",
    "\n",
    "roc_visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_visualizer.show()                       # Finalize and show the figure\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(RFCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()\n",
    "\n",
    "# Create the Precision-Recall visualizer, fit, score, and show it\n",
    "prc_viz = PrecisionRecallCurve(RFCmodel)\n",
    "prc_viz.fit(X_train, y_train)\n",
    "prc_viz.score(X_test, y_test)\n",
    "prc_viz.show()\n",
    "\n",
    "# Instantiate the classification model and Class Prediction Error visualizer\n",
    "cpe_visualizer = ClassPredictionError(\n",
    "    RFCmodel, classes=classes\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "cpe_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "cpe_visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "cpe_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "RFCoptimal = RandomForestClassifier()\n",
    "\n",
    "min_samples_split = np.linspace(0.5,2.5,5)\n",
    "max_depth = [int(x) for x in np.linspace(1,10,10)]\n",
    "criterion = ['gini', 'entropy']\n",
    "n_estimators = [int(x) for x in np.linspace(100,150,2)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "params = dict(max_depth = max_depth,  \n",
    "             min_samples_split = min_samples_split)  \n",
    "             criterion = criterion, \n",
    "             n_estimators = n_estimators,\n",
    "             max_features = max_features)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(RFCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, RFCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "#splits = train_test_split(x, y, test_size=0.2)\n",
    "#X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "GBCmodel = GradientBoostingClassifier()\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(GBCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(GBCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Instantiate the ROC visualizer with the classification model\n",
    "roc_visualizer = ROCAUC(GBCmodel, classes=classes)\n",
    "\n",
    "roc_visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_visualizer.show()                       # Finalize and show the figure\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(GBCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()\n",
    "\n",
    "# Create the Precision-Recall visualizer, fit, score, and show it\n",
    "prc_viz = PrecisionRecallCurve(GBCmodel)\n",
    "prc_viz.fit(X_train, y_train)\n",
    "prc_viz.score(X_test, y_test)\n",
    "prc_viz.show()\n",
    "\n",
    "# Instantiate the classification model and Class Prediction Error visualizer\n",
    "cpe_visualizer = ClassPredictionError(\n",
    "    GBCmodel, classes=classes\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "cpe_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "cpe_visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "cpe_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "GBCoptimal = GradientBoostingClassifier(max_features = 'sqrt', criterion = 'mae', max_depth = 4, validation_fraction = 0.675, subsample = 0.96875)\n",
    "\n",
    "learning_rate = np.linspace(0.1, 1, 5)\n",
    "max_depth = [int(x) for x in np.linspace(1,5,5)]\n",
    "min_impurity_decrease = np.linspace(0,1,5)\n",
    "validation_fraction = np.linspace(0.65,0.7,5)\n",
    "min_samples_leaf = [1, 2, 3]\n",
    "subsample = np.linspace(0.875, 1, 5)\n",
    "\n",
    "params = dict(learning_rate = learning_rate,\n",
    "              subsample = subsample,\n",
    "              min_samples_leaf = min_samples_leaf,\n",
    "              validation_fraction = validation_fraction,\n",
    "              min_weight_fraction_leaf = min_weight_fraction_leaf,\n",
    "              max_depth = max_depth)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(GBCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, GBCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "#splits = train_test_split(x, y, test_size=0.2)\n",
    "#X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "DTCinput = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, min_samples_split = 0.2)\n",
    "GNBinput = GaussianNB()\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier()\n",
    "GBCinput = GradientBoostingClassifier()\n",
    "\n",
    "VCmodel = VotingClassifier(estimators = [('dtc', DTCinput), ('gnb', GNBinput), ('rfc', RFCinput), ('etc', ETCinput), ('gbc', GBCinput)], voting = 'hard')\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(VCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(VCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(VCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()\n",
    "\n",
    "# Create the Precision-Recall visualizer, fit, score, and show it\n",
    "prc_viz = PrecisionRecallCurve(VCmodel)\n",
    "prc_viz.fit(X_train, y_train)\n",
    "prc_viz.score(X_test, y_test)\n",
    "prc_viz.show()\n",
    "\n",
    "# Instantiate the classification model and Class Prediction Error visualizer\n",
    "cpe_visualizer = ClassPredictionError(\n",
    "    VCmodel, classes=classes\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "cpe_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "cpe_visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "cpe_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "DTCinput = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, min_samples_split = 0.2)\n",
    "GNBinput = GaussianNB()\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier()\n",
    "GBCinput = GradientBoostingClassifier()\n",
    "BCinput = BaggingClassifier(GaussianNB())\n",
    "ABCinput = AdaBoostClassifier(RandomForestClassifier())\n",
    "SCinput = StackingClassifier(estimators = [('dtc', DTCinput), ('gnb', GNBinput), ('rfc', RFCinput), ('etc', ETCinput), ('gbc', GBCinput), ('bc', BCinput), ('abc', ABCinput)], final_estimator = LogisticRegression(), cv = 12)\n",
    "\n",
    "VCoptimal = VotingClassifier(estimators = [('dtc', DTCinput), ('gnb', GNBinput), ('rfc', RFCinput), ('etc', ETCinput), ('gbc', GBCinput), ('bc', BCinput), ('abc', ABCinput), ('sc', SCinput)])\n",
    "\n",
    "voting = ['soft', 'hard']\n",
    "flatten_transform = [True, False]\n",
    "\n",
    "params = dict(voting = voting,\n",
    "              flatten_transform = flatten_transform)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(VCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, VCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "BCmodel = BaggingClassifier(GNBmodel)\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(BCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(BCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Instantiate the ROC visualizer with the classification model\n",
    "roc_visualizer = ROCAUC(BCmodel, classes=classes)\n",
    "\n",
    "roc_visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_visualizer.show()                       # Finalize and show the figure\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(DTCmodel, classes=classes, cmap = 'BuPu')\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()\n",
    "\n",
    "# Create the Precision-Recall visualizer, fit, score, and show it\n",
    "prc_viz = PrecisionRecallCurve(BCmodel)\n",
    "prc_viz.fit(X_train, y_train)\n",
    "prc_viz.score(X_test, y_test)\n",
    "prc_viz.show()\n",
    "\n",
    "# Instantiate the classification model and Class Prediction Error visualizer\n",
    "cpe_visualizer = ClassPredictionError(\n",
    "    BCmodel, classes=classes\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "cpe_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "cpe_visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "cpe_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "BCoptimal = BaggingClassifier()\n",
    "\n",
    "DTCinput = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, min_samples_split = 0.2)\n",
    "GNBinput = GaussianNB()\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier()\n",
    "GBCinput = GradientBoostingClassifier(criterion = 'mae', max_features = 'sqrt', max_depth = 4, validation_fraction = 0.675, subsample = 0.96875)\n",
    "\n",
    "base_estimator = [DTCinput, GNBinput, RFCinput, ETCinput, GBCinput]\n",
    "n_estimators = [int(x) for x in np.linspace(10,100,10)]\n",
    "#max_samples = np.linspace(0.1, 1, 5)\n",
    "max_features = np.linspace(0.1, 1, 5)\n",
    "#bootstrap = [True, False]\n",
    "#bootstrap_features = [True, False]\n",
    "#oob_score = [True, False]\n",
    "#warm_start = [True, False]\n",
    "\n",
    "params = dict(base_estimator = base_estimator,\n",
    "             n_estimators = n_estimators,\n",
    "             #max_samples = max_samples,  \n",
    "             max_features = max_features)\n",
    "             #bootstrap = bootstrap,\n",
    "             #bootstrap_features = bootstrap_features,\n",
    "             #oob_score = oob_score,\n",
    "             #warm_start = warm_start)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(BCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, BCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "ABCmodel = AdaBoostClassifier(RFCmodel)\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(ABCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(ABCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Instantiate the ROC visualizer with the classification model\n",
    "roc_visualizer = ROCAUC(ABCmodel, classes=classes)\n",
    "\n",
    "roc_visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_visualizer.show()                       # Finalize and show the figure\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(ABCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()\n",
    "\n",
    "# Create the Precision-Recall visualizer, fit, score, and show it\n",
    "prc_viz = PrecisionRecallCurve(ABCmodel)\n",
    "prc_viz.fit(X_train, y_train)\n",
    "prc_viz.score(X_test, y_test)\n",
    "prc_viz.show()\n",
    "\n",
    "# Instantiate the classification model and Class Prediction Error visualizer\n",
    "cpe_visualizer = ClassPredictionError(\n",
    "    ABCmodel, classes=classes\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "cpe_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "cpe_visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "cpe_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "ABCoptimal = AdaBoostClassifier()\n",
    "\n",
    "DTCinput = DecisionTreeClassifier()\n",
    "GNBinput = GaussianNB()\n",
    "KNNCinput = KNeighborsClassifier()\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier\n",
    "GBCinput = GradientBoostingClassifier(criterion = 'mae', max_features = 'sqrt', max_depth = 4, validation_fraction = 0.675, subsample = 0.96875)\n",
    "BCinput = BaggingClassifier(GaussianNB())\n",
    "\n",
    "base_estimator = [RFCinput]\n",
    "n_estimators = [int(x) for x in np.linspace(10,100,10)]\n",
    "learning_rate = np.linspace(0.1, 1, 5)\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "\n",
    "params = dict(base_estimator = base_estimator, \n",
    "             n_estimators = n_estimators,\n",
    "             learning_rate = learning_rate,  \n",
    "             algorithm = algorithm)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(ABCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, ABCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "DTCinput = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, min_samples_split = 0.2)\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier()\n",
    "GNBinput = GaussianNB()\n",
    "GBCinput = GradientBoostingClassifier()\n",
    "BCinput = BaggingClassifier(GNBmodel)\n",
    "ABCinput = AdaBoostClassifier(RandomForestClassifier())\n",
    "\n",
    "estimators = [('dtc', DTCinput), ('gnb', GNBinput), ('rfc', RFCinput), ('etc', ETCinput), ('gbc', GBCinput), ('bc', BCinput), ('abc', ABCinput)]\n",
    "\n",
    "SCmodel = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression(), cv = 12, stack_method = 'auto')\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(SCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(SCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Instantiate the ROC visualizer with the classification model\n",
    "roc_visualizer = ROCAUC(SCmodel, classes=classes)\n",
    "\n",
    "roc_visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "roc_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "roc_visualizer.show()                       # Finalize and show the figure\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(SCmodel, classes=classes, cmap='BuPu')\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()\n",
    "\n",
    "# Create the Precision-Recall visualizer, fit, score, and show it\n",
    "prc_viz = PrecisionRecallCurve(SCmodel)\n",
    "prc_viz.fit(X_train, y_train)\n",
    "prc_viz.score(X_test, y_test)\n",
    "prc_viz.show()\n",
    "\n",
    "# Instantiate the classification model and Class Prediction Error visualizer\n",
    "cpe_visualizer = ClassPredictionError(\n",
    "    SCmodel, classes=classes\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "cpe_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "cpe_visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "cpe_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DTCinput = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, min_samples_split = 0.2)\n",
    "GNBinput = GaussianNB()\n",
    "KNNCinput = KNeighborsClassifier(algorithm = 'auto', leaf_size = 5, n_neighbors = 100, weights = 'uniform')\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier()\n",
    "GBCinput = GradientBoostingClassifier(criterion = 'mae', max_features = 'sqrt', max_depth = 4, validation_fraction = 0.675, subsample = 0.96875)\n",
    "BCinput = BaggingClassifier(GaussianNB(), max_features = 0.325, n_estimators = 70)\n",
    "ABCinput = AdaBoostClassifier(RandomForestClassifier(), learning_rate = 0.325, n_estimators = 10, algorithm = 'SAMME')\n",
    "\n",
    "estimators = [('dtc', DTCinput), ('gnb', GNBinput), ('knnc', KNNCinput), ('rfc', RFCinput), ('etc', ETCinput), ('gbc', GBCinput), ('bc', BCinput), ('abc', ABCinput)]\n",
    "\n",
    "SCoptimal = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression())\n",
    "\n",
    "stack_method = ['auto', 'predict_proba', 'decision_function', 'predict']\n",
    "\n",
    "params = dict(stack_method = stack_method)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(SCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, SCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
