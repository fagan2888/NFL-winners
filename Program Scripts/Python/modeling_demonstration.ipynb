{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "import pandas as pds\n",
    "\n",
    "import sqlalchemy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as scipy\n",
    "\n",
    "import statistics as stats\n",
    "\n",
    "import math as math\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import yellowbrick as yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Database Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engine = sqlalchemy.create_engine('postgresql+psycopg2://postgres:georgetown@nflstats.cb6meldrm5db.us-east-1.rds.amazonaws.com:5432/nfl_stats', pool_recycle=3600);\n",
    "\n",
    "dbConnection = Engine.connect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe and QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pds.read_sql(\"\"\"select * from final_table_joined\"\"\", dbConnection);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_table.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for RFC Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['home_point_differential', 'home_passing_yards', 'home_rushing_yards', 'home_turnover_differential', 'home_passing_yards_against', 'home_rushing_yards_against', 'home_win_percentage', 'home_power_ranking', 'away_point_differential', 'away_passing_yards', 'away_rushing_yards', 'away_turnover_differential', 'away_passing_yards_against', 'away_rushing_yards_against', 'away_win_percentage', 'away_power_ranking']\n",
    "x = df_table[features].values\n",
    "y = df_table['home_outcome'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "DTCmodel = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, min_samples_split = 0.2)\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(DTCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(DTCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(DTCmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(DTCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DTCoptimal = DecisionTreeClassifier()\n",
    "\n",
    "min_samples_split = np.linspace(0.1,0.5,5)\n",
    "max_depth = [int(x) for x in np.linspace(1,10,1)]\n",
    "criterion = ['gini', 'entropy']\n",
    "\n",
    "params = dict(max_depth = max_depth,  \n",
    "             min_samples_split = min_samples_split,  \n",
    "             criterion = criterion)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(DTCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, DTCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "GNBmodel = GaussianNB()\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(GNBmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(GNBmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(GNBmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(GNBmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "KNNCmodel = KNeighborsClassifier(algorithm = 'auto', leaf_size = 5, n_neighbors = 100, weights = 'uniform')\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(KNNCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(KNNCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(KNNCmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(KNNCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "KNNCoptimal = KNeighborsClassifier()\n",
    "\n",
    "n_neighbors = [int(x) for x in np.linspace(5,100,20)]\n",
    "weights = ['uniform', 'distance']\n",
    "algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "leaf_size = [int(x) for x in np.linspace(5,100,20)]\n",
    "\n",
    "params = dict(n_neighbors = n_neighbors,  \n",
    "             weights = weights,  \n",
    "             algorithm = algorithm,\n",
    "             leaf_size = leaf_size)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(KNNCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, KNNCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "RFCmodel = RandomForestClassifier()\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(RFCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(RFCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(RFCmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(RFCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "RFCoptimal = RandomForestClassifier()\n",
    "\n",
    "min_samples_split = np.linspace(0.1,0.5,5)\n",
    "max_depth = [int(x) for x in np.linspace(1,10,10)]\n",
    "criterion = ['gini', 'entropy']\n",
    "n_estimators = [int(x) for x in np.linspace(10,200,20)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "params = dict(max_depth = max_depth,  \n",
    "             min_samples_split = min_samples_split,  \n",
    "             criterion = criterion, n_estimators = n_estimators, \n",
    "             max_features = max_features)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(RFCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, RFCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "ETCmodel = ExtraTreesClassifier()\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(ETCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(ETCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(ETCmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(ETCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ETCoptimal = ExtraTreesClassifier()\n",
    "\n",
    "min_samples_split = np.linspace(0.1,0.5,5)\n",
    "max_depth = [int(x) for x in np.linspace(1,20,10)]\n",
    "criterion = ['gini', 'entropy']\n",
    "n_estimators = [int(x) for x in np.linspace(10,200,20)]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "params = dict(max_depth = max_depth,  \n",
    "             min_samples_split = min_samples_split,  \n",
    "             criterion = criterion, n_estimators = n_estimators, \n",
    "             max_features = max_features)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(RFCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, ETCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "GBCmodel = GradientBoostingClassifier()\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(GBCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(GBCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(GBCmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(GBCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "GBCoptimal = GradientBoostingClassifier()\n",
    "\n",
    "loss = ['deviance', 'exponential']\n",
    "learning_rate = np.linspace(0.1, 1, 5)\n",
    "n_estimators = [int(x) for x in np.linspace(50,1000,5)]\n",
    "max_depth = [int(x) for x in np.linspace(5,100,5)]\n",
    "max_features = ['log2', 'sqrt']\n",
    "criterion = ['friedman_mse', 'mae']\n",
    "\n",
    "params = dict(loss = loss, learning_rate = learning_rate, n_estimators = n_estimators,   \n",
    "             max_depth = max_depth,max_features = max_features, \n",
    "             criterion = criterion)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(GBCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, GBCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "DTCinput = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, min_samples_split = 0.2)\n",
    "GNBinput = GaussianNB()\n",
    "KNNCinput = KNeighborsClassifier(algorithm = 'auto', leaf_size = 5, n_neighbors = 100, weights = 'uniform')\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier()\n",
    "GBCinput = GradientBoostingClassifier()\n",
    "\n",
    "VCmodel = VotingClassifier(estimators = [('dtc', DTCinput), ('gnb', GNBinput), ('knnc', KNNCinput), ('rfc', RFCinput), ('etc', ETCinput), ('gbc', GBCinput)], voting = 'hard')\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(VCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(VCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(VCmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(VCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "BCmodel = BaggingClassifier(GBCmodel)\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(BCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(BCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(BCmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(BCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "BCoptimal = BagginingClassifier()\n",
    "\n",
    "base_estimator = [DTCinput, GNBinput, RFCinput, ETCinput, GBCinput]\n",
    "n_estimators = [int(x) for x in np.linspace(10,100,10)]\n",
    "max_samples = np.linspace(0.1, 1, 5)\n",
    "max_features = np.linspace(0.1, 1, 5)\n",
    "bootstrap = [True, False]\n",
    "bootstrap_features = [True, False]\n",
    "oob_score = [True, False]\n",
    "warm_start = [True, False]\n",
    "\n",
    "params = dict(base_estimator = base_estimator,\n",
    "             n_estimators = n_estimators,  \n",
    "             max_samples = max_samples,  \n",
    "             max_features = max_features,\n",
    "             bootstrap = bootstrap,\n",
    "             bootstrap_features = bootstrap_features,\n",
    "             oob_score = oob_score,\n",
    "             warm_start = warm_start)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(BCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, BCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "ABCmodel = AdaBoostClassifier(GBCmodel)\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(ABCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(ABCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(ABCmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(ABCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ABCoptimal = AdaBoostClassifier()\n",
    "\n",
    "DTCinput = DecisionTreeClassifier()\n",
    "GNBinput = GaussianNB()\n",
    "KNNCinput = KNeighborsClassifier()\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier\n",
    "GBCinput = GradientBoostingClassifier()\n",
    "\n",
    "base_estimator = [DTCinput, GNBinput, KNNCinput, RFCinput, ETCinput, GBCinput]\n",
    "n_estimators = [int(x) for x in np.linspace(5,100,20)]\n",
    "learning_rate = np.linspace(0.1, 1, 10)\n",
    "algorithm = ['SAMME', 'SAMME.R']\n",
    "\n",
    "params = dict(base_estimator = base_estimator, \n",
    "             n_estimators = n_estimators,  \n",
    "             learning_rate = learning_rate,  \n",
    "             algorithm = algorithm)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(ABCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, ABCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.model_selection import CVScores\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "splits = train_test_split(x, y, test_size=0.2)\n",
    "X_train, X_test, y_train, y_test = splits\n",
    "\n",
    "DTCinput = DecisionTreeClassifier()\n",
    "GNBinput = GaussianNB()\n",
    "KNNCinput = KNeighborsClassifier()\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier()\n",
    "GBCinput = GradientBoostingClassifier()\n",
    "\n",
    "estimators = [('dtc', DTCinput), ('gnb', GNBinput), ('knnc', KNNCinput), ('rfc', RFCinput), ('etc', ETCinput), ('gbc', GBCinput)]\n",
    "\n",
    "SCmodel = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression(), cv = 12, stack_method = 'auto')\n",
    "\n",
    "classes = ['loss', 'win']\n",
    "\n",
    "# Classification Report\n",
    "cr_visualizer = ClassificationReport(SCmodel, classes=classes, support=True)\n",
    "\n",
    "cr_visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "cr_visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "cr_visualizer.show()  \n",
    "\n",
    "# Cross-Validation\n",
    "cv = StratifiedKFold(n_splits = 12)\n",
    "\n",
    "cv_visualizer = CVScores(SCmodel, cv = cv, scoring = 'f1_weighted')\n",
    "\n",
    "cv_visualizer.fit(x, y)\n",
    "cv_visualizer.show()\n",
    "\n",
    "# Learning Curve\n",
    "sizes = np.linspace(0.1, 1, 20)\n",
    "\n",
    "lc_visualizer = LearningCurve(SCmodel, cv = cv, scoring='f1_weighted', train_sizes = sizes)\n",
    "\n",
    "lc_visualizer.fit(x, y)\n",
    "lc_visualizer.show()\n",
    "\n",
    "# The ConfusionMatrix visualizer taxes a model\n",
    "cm_visualizer = ConfusionMatrix(SCmodel, classes=classes)\n",
    "\n",
    "# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\n",
    "cm_visualizer.fit(X_train, y_train)\n",
    "\n",
    "# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n",
    "# and then creates the confusion_matrix from scikit-learn.\n",
    "cm_visualizer.score(X_test, y_test)\n",
    "\n",
    "# How did we do?\n",
    "cm_visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "DTCinput = DecisionTreeClassifier()\n",
    "GNBinput = GaussianNB()\n",
    "KNNCinput = KNeighborsClassifier()\n",
    "RFCinput = RandomForestClassifier()\n",
    "ETCinput = ExtraTreesClassifier()\n",
    "GBCinput = GradientBoostingClassifier()\n",
    "\n",
    "estimators = [('dtc', DTCinput), ('gnb', GNBinput), ('knnc', KNNCinput), ('rfc', RFCinput), ('etc', ETCinput), ('gbc', GBCinput)]\n",
    "\n",
    "SCoptimal = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression())\n",
    "\n",
    "stack_method = ['auto', 'predict_proba', 'decision_function', 'predict']\n",
    "\n",
    "params = dict(stack_method = stack_method)\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "\n",
    "    gridF = GridSearchCV(SCoptimal, params, cv = 12, verbose = 1, \n",
    "                      n_jobs = -1)\n",
    "\n",
    "    bestF = gridF.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(bestF.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = bestF.cv_results_['mean_test_score']\n",
    "    stds = bestF.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, bestF.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, SCoptimal.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbConnection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
